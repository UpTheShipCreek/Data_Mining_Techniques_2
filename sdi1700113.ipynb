{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4f9d13d",
      "metadata": {
        "id": "e4f9d13d"
      },
      "source": [
        "# # <div style=\"text-align:center\"> Τεχνικές Εξόρυξης Δεδομένων: 2η Άσκηση  </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d05e85",
      "metadata": {
        "id": "18d05e85"
      },
      "source": [
        "### <div style=\"text-align:center\">  Παναγιώτοπουλος Γεώργιος **1115201700113** </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac1fa2df",
      "metadata": {
        "id": "ac1fa2df"
      },
      "source": [
        "Αρχικά θα εισάγετε όλο το αρχείο σε ένα dataframe και θα μελετήσετε αν υπάρχουν τιμές Nan\n",
        "στις στήλες ώστε να αφαιρέσετε αυτές τις γραμμες από το dataframe. Στην συνέχεια θα γράψετε\n",
        "την κατάλληλες εντολές σε python για να απαντήσετε στα παρακάτω ζητούμενα. Τι περισσότερες\n",
        "φορές οι απαντήσει θα δίνονται με ένα γράφημα και μπορείτε να χρησιμοποιήσετε όποια python\n",
        "βιβλιοθήκη θέλετε για το σκοπό αυτό."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f5581f96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5581f96",
        "outputId": "f7282c01-24c2-46ad-b04e-40319784d659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 52478 entries, 0 to 52477\n",
            "Data columns (total 25 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   bookId            52478 non-null  object \n",
            " 1   title             52478 non-null  object \n",
            " 2   series            23470 non-null  object \n",
            " 3   author            52478 non-null  object \n",
            " 4   rating            52478 non-null  float64\n",
            " 5   description       51140 non-null  object \n",
            " 6   language          48672 non-null  object \n",
            " 7   isbn              52478 non-null  object \n",
            " 8   genres            52478 non-null  object \n",
            " 9   characters        52478 non-null  object \n",
            " 10  bookFormat        51005 non-null  object \n",
            " 11  edition           4955 non-null   object \n",
            " 12  pages             50131 non-null  object \n",
            " 13  publisher         48782 non-null  object \n",
            " 14  publishDate       51598 non-null  object \n",
            " 15  firstPublishDate  31152 non-null  object \n",
            " 16  awards            52478 non-null  object \n",
            " 17  numRatings        52478 non-null  int64  \n",
            " 18  ratingsByStars    52478 non-null  object \n",
            " 19  likedPercent      51856 non-null  float64\n",
            " 20  setting           52478 non-null  object \n",
            " 21  coverImg          51873 non-null  object \n",
            " 22  bbeScore          52478 non-null  int64  \n",
            " 23  bbeVotes          52478 non-null  int64  \n",
            " 24  price             38113 non-null  object \n",
            "dtypes: float64(2), int64(3), object(20)\n",
            "memory usage: 10.0+ MB\n",
            "Number of null values in 'title' column before cleaning: 29008\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/')\n",
        "\n",
        "# df = pd.read_csv('./books_1.Best_Books_Ever.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/books_1.Best_Books_Ever.csv')\n",
        "df.info(verbose=True)\n",
        "print(\"Number of null values in 'title' column before cleaning:\", df['series'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c710bbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c710bbf",
        "outputId": "e10e9791-804e-4547-a2f0-1fc7f90ed72e",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27121 entries, 0 to 27120\n",
            "Data columns (total 25 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   bookId            27121 non-null  object \n",
            " 1   title             27121 non-null  object \n",
            " 2   series            27121 non-null  object \n",
            " 3   author            27121 non-null  object \n",
            " 4   rating            27121 non-null  float64\n",
            " 5   description       27121 non-null  object \n",
            " 6   language          27121 non-null  object \n",
            " 7   isbn              27121 non-null  object \n",
            " 8   genres            27121 non-null  object \n",
            " 9   characters        27121 non-null  object \n",
            " 10  bookFormat        27121 non-null  object \n",
            " 11  edition           27121 non-null  object \n",
            " 12  pages             27121 non-null  object \n",
            " 13  publisher         27121 non-null  object \n",
            " 14  publishDate       27121 non-null  object \n",
            " 15  firstPublishDate  27121 non-null  object \n",
            " 16  awards            27121 non-null  object \n",
            " 17  numRatings        27121 non-null  int64  \n",
            " 18  ratingsByStars    27121 non-null  object \n",
            " 19  likedPercent      27121 non-null  float64\n",
            " 20  setting           27121 non-null  object \n",
            " 21  coverImg          27121 non-null  object \n",
            " 22  bbeScore          27121 non-null  int64  \n",
            " 23  bbeVotes          27121 non-null  int64  \n",
            " 24  price             27121 non-null  float64\n",
            "dtypes: float64(3), int64(3), object(19)\n",
            "memory usage: 5.2+ MB\n"
          ]
        }
      ],
      "source": [
        "# Μπορούμε να βάλουμε τo median price σε όσα βιβλία δεν εχουν κάποια τίμη\n",
        "import re\n",
        "def fix_double_dots(string): #Some of the prices have three `.` for some reason\n",
        "    pattern = r'^(\\w+)\\.(\\w+)\\.(\\w+)$'\n",
        "    match = re.match(pattern, string)\n",
        "    if match:\n",
        "        return match.group(1) + match.group(2) + '.' + match.group(3)\n",
        "    else:\n",
        "        return string\n",
        "    \n",
        "prices = []\n",
        "for i in range(len(df.index)):\n",
        "    if isinstance(df['price'][i],str):\n",
        "        price = fix_double_dots(df['price'][i])\n",
        "    else:\n",
        "        price = df['price'][i]\n",
        "    prices.append(float(price))      \n",
        "df['price'] = prices\n",
        "price_median = df['price'].median()\n",
        "df['price'].fillna(price_median, inplace=True)\n",
        "\n",
        "# Επίσης μπορόυμε να υποθέσουμε ότι για τα βιβλία που δεν αναγράφεται edition,ισχύει το Standard edition\n",
        "df['edition'].fillna('Standard Edition',inplace=True)\n",
        "\n",
        "# Και ότι για όσα δεν αναγράφεται Series, ΄ότι είναι Standalone\n",
        "df['series'].fillna('Standalone',inplace=True)\n",
        "\n",
        "df_clean = df.dropna().reset_index(drop=True)\n",
        "df_clean.info(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58bcfd3",
      "metadata": {
        "id": "a58bcfd3"
      },
      "source": [
        "## 1) Προεπεξεργασία (10%) \n",
        "Παρατηρήστε την στήλη ratingsByStars, περιέχει 5 τιμές χωρισμένες με κόμματα , χωρίστε τις\n",
        "τιμές αυτές και προσθέστε στο dataframe ξεχωριστά τα ratings, δηλαδή ratingStar5, ratingStar4,\n",
        "ratingStar3 κτλ.\n",
        "Επίσης η στήλη genres περιέχει για κάθε βιβλίο περισσότερα από ένα genre (είδος).\n",
        "Δημιουργήστε μία νέα στήλη (ονομάστε την genreSingle) και βάλτε μόνο το πρώτο genre από\n",
        "όλα τα genres που συναντάμε σε κάθε γραμμή. (πχ ['Fantasy', 'Young Adult', 'Fiction', 'Magic',\n",
        "'Childrens', 'Adventure', 'Audiobook', 'Middle Grade', 'Classics', 'Science Fiction Fantasy'] -> η\n",
        "νέα στήλη θα έχει το 'Fantasy' ) deleting books without any genre information.\n",
        "Χρησιμοποιήστε την στήλη publishDate και δημιουργήστε μία νέα στήλη με το έτος έκδοσης κάθε\n",
        "βιβλίου (μπορείτε να χρησιμοποιήστε την μέθοδο to_datetime() που παρέχει το pandas ή ότι\n",
        "άλλο θέλετε) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9b9eed4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b9eed4a",
        "outputId": "e113eeba-e397-4bd3-9c3d-d80c5ce49aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser) (2022.7.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2022.10.31)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser) (4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser) (1.16.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser) (2023.3)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.1.8\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26889 entries, 0 to 26888\n",
            "Data columns (total 32 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   bookId            26889 non-null  object \n",
            " 1   title             26889 non-null  object \n",
            " 2   series            26889 non-null  object \n",
            " 3   author            26889 non-null  object \n",
            " 4   rating            26889 non-null  float64\n",
            " 5   description       26889 non-null  object \n",
            " 6   language          26889 non-null  object \n",
            " 7   isbn              26889 non-null  object \n",
            " 8   genres            26889 non-null  object \n",
            " 9   characters        26889 non-null  object \n",
            " 10  bookFormat        26889 non-null  object \n",
            " 11  edition           26889 non-null  object \n",
            " 12  pages             26889 non-null  object \n",
            " 13  publisher         26889 non-null  object \n",
            " 14  publishDate       26889 non-null  object \n",
            " 15  firstPublishDate  26889 non-null  object \n",
            " 16  awards            26889 non-null  object \n",
            " 17  numRatings        26889 non-null  int64  \n",
            " 18  ratingsByStars    26889 non-null  object \n",
            " 19  likedPercent      26889 non-null  float64\n",
            " 20  setting           26889 non-null  object \n",
            " 21  coverImg          26889 non-null  object \n",
            " 22  bbeScore          26889 non-null  int64  \n",
            " 23  bbeVotes          26889 non-null  int64  \n",
            " 24  price             26889 non-null  float64\n",
            " 25  ratingStar5       26889 non-null  object \n",
            " 26  ratingStar4       26889 non-null  object \n",
            " 27  ratingStar3       26889 non-null  object \n",
            " 28  ratingStar2       26889 non-null  object \n",
            " 29  ratingStar1       26889 non-null  object \n",
            " 30  genreSingle       26889 non-null  object \n",
            " 31  publishYear       26889 non-null  float64\n",
            "dtypes: float64(4), int64(3), object(25)\n",
            "memory usage: 6.6+ MB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "!pip install dateparser\n",
        "import dateparser\n",
        "\n",
        "#rows = len(df_clean.index)\n",
        "\n",
        "# Useful function for getting rid of trash symbols\n",
        "def remove_trash_symbols(string,symbols):\n",
        "    for symbol in symbols:\n",
        "            string = string.replace(symbol, \"\")\n",
        "    return string\n",
        "\n",
        "# Seperating rating entries\n",
        "symbols_to_remove = \"[]''\"\n",
        "clean = []\n",
        "reformatted = []\n",
        "for i in range(len(df_clean.index)): # For the ratings of every book\n",
        "    entry = df_clean['ratingsByStars'][i]\n",
        "    if(entry == \"[]\"): # some completely empty entries for some reason exist\n",
        "        entry = \"[0,0,0,0,0]\"\n",
        "    formatted = list(entry.split(\",\")) # split the string in the comas\n",
        "    for rating in formatted: # and for every element of the broken form ['1593642 left\n",
        "        string_left = remove_trash_symbols(rating,symbols_to_remove)\n",
        "        clean.append(string_left)\n",
        "\n",
        "reformatted = np.array(clean).reshape(len(df_clean.index),5)\n",
        "\n",
        "# Adding the rows of the reformatted array into the dataframe\n",
        "df_clean['ratingStar5'] = reformatted[:,0]\n",
        "df_clean['ratingStar4'] = reformatted[:,1]\n",
        "df_clean['ratingStar3'] = reformatted[:,2]\n",
        "df_clean['ratingStar2'] = reformatted[:,3]\n",
        "df_clean['ratingStar1'] = reformatted[:,4]\n",
        "\n",
        "# Seperating genre\n",
        "first_genre = []\n",
        "for i in range(len(df_clean.index)): # For the ratings of every book\n",
        "    entry = df_clean['genres'][i]\n",
        "    first_genre.append(remove_trash_symbols(entry.split(',')[0],symbols_to_remove))\n",
        "# Adding the new main genre column\n",
        "df_clean['genreSingle'] = first_genre\n",
        "\n",
        "years = []\n",
        "for date in df_clean['publishDate']:\n",
        "    parsed_date = dateparser.parse(date)\n",
        "    if parsed_date is not None:   # cause a nonetype object doesn't have any attributes\n",
        "        years.append(parsed_date.year)\n",
        "    else:  \n",
        "        years.append(None) \n",
        "df_clean['publishYear'] = years\n",
        "\n",
        "df_clean = df_clean.dropna().reset_index(drop=True)\n",
        "df_clean.info(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cac866",
      "metadata": {
        "id": "26cac866"
      },
      "source": [
        "## 2) Ερωτήματα για μελέτη των δεδομένων - απαντήστε σε 5 από τα παρακάτω (20%)\n",
        "1. Κατασκευάστε το ιστόγραμμα των ratings στο σύνολο δεδομένων (χρησιμοποιήστε την\n",
        "στήλη rating)\n",
        "2. Ποιά είναι τα 10 βιβλία με τις περισσότερες σελίδες.\n",
        "3. Ποιά είναι τα 10 βιβλία με τα περισσότερα 5-αστέρια (χρησιμοποιήστε μόνο τα βιβλία που\n",
        "έχουν λάβει πάνω από 10.000 5-star ratings από τη στήλη ratingStar5) .\n",
        "4. Ποιές είναι οι πιο συχνές λέξεις στους τίτλους των βιβλίων (αφού αφαιρεθούν τα stop\n",
        "words)\n",
        "5. Ποιοι είναι οι 10 συγγραφεις με τα περισσότερα βιβλία\n",
        "6. Ποιοι είναι οι 10 συγγραφείς με τις περισσότερες κριτικές (χρησιμοποιήστε την στήλη\n",
        "numRatings).\n",
        "7. Κατατάξτε του συγγραφείς με βάση τα βιβλία τους ανά έτος.\n",
        "8. Ποιές είναι οι πιο συχνές γλώσσες που έχουν γραφτεί τα βιβλία στα δεδομένα σας\n",
        "9. Ποιοί είναι οι 10 εκδότες που έχουν εκδώσει τα περισσότερα βιβλία.\n",
        "10. Έχουν τα βιβλία με τις περισσότερες σελίδες (πχ περισσότερες από 1000 pages)\n",
        "υψηλότερα ratings ?\n",
        "11. Συγκεντρώστε σε ένα γράφημα η σε ένα πίνακα όλα τα μοναδικά είδη βιβλίων (genres).\n",
        "Ποιά είναι τα πιο συχνά genres; Το ίδιο και για τα awards.\n",
        "12. Φτιάξτε τα wordclouds για τη στήλη description. Σε αυτό το ερώτημα αφαιρέστε τα stop\n",
        "words, πειραματιστείτε με τις παραμέτρους του wordcloud και εντοπίστε τις πιο\n",
        "χαρακτηριστικές λέξεις που χρησιμοποιούνται στα βιβλία του συνόλου των δεδομένων\n",
        "σας.\n",
        "13. Πόσα βιβλία εκδίδονται ανά έτος ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "20fd4406",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fd4406",
        "outputId": "988cd578-c0e3-4f8d-eccd-fb11528af924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books with most pages: \n",
            " 11961                        7506252-are-you-kidding-me\n",
            "14779                   591965.The_Madwoman_of_Chaillot\n",
            "16039                                    875775.Mission\n",
            "12187                       131094.Millroy_the_Magician\n",
            "18666                                      2581254-halo\n",
            "16150    10394216-the-complete-works-of-charles-dickens\n",
            "16265                          10168286-the-village-wit\n",
            "9482                       16312.One_Two_Buckle_My_Shoe\n",
            "18514                         121646.Murder_in_the_Mews\n",
            "12052                      7058894-12-stocking-stuffers\n",
            "Name: bookId, dtype: object\n",
            "Books with most 5 star ratings: \n",
            " 11063            15747245-close-your-eyes-and-see\n",
            "15460                        2338189.Zany_Hijinx_\n",
            "10976    12813058-lost-souls---the-cube-of-asgard\n",
            "11021              13560868-the-language-of-light\n",
            "13159        14643361-telling-time-by-the-shadows\n",
            "13790                        2583110-plural-loves\n",
            "11127                            3454581-the-list\n",
            "11167                   17455034-sign-of-the-time\n",
            "11176           14535907-jesus-and-moses-in-india\n",
            "15279                       1608094.Nuclear_Peace\n",
            "Name: bookId, dtype: object\n",
            "Authors with most books: \n",
            " author\n",
            "Agatha Christie                       68\n",
            "Terry Pratchett                       43\n",
            "Nora Roberts (Goodreads Author)       41\n",
            "Stephen King (Goodreads Author)       40\n",
            "Enid Blyton                           40\n",
            "Karen Kingsbury (Goodreads Author)    36\n",
            "Dean Koontz (Goodreads Author)        36\n",
            "P.G. Wodehouse                        33\n",
            "Orson Scott Card                      33\n",
            "Piers Anthony                         31\n",
            "Name: bookId, dtype: int64\n",
            "Authors with most ratings: \n",
            " author\n",
            "Agatha Christie                       68\n",
            "Terry Pratchett                       43\n",
            "Nora Roberts (Goodreads Author)       41\n",
            "Stephen King (Goodreads Author)       40\n",
            "Enid Blyton                           40\n",
            "Karen Kingsbury (Goodreads Author)    36\n",
            "Dean Koontz (Goodreads Author)        36\n",
            "P.G. Wodehouse                        33\n",
            "Orson Scott Card                      33\n",
            "Piers Anthony                         31\n",
            "Name: numRatings, dtype: int64\n",
            "Most popular languages: \n",
            " language\n",
            "English       24057\n",
            "Spanish         390\n",
            "French          356\n",
            "Arabic          348\n",
            "German          280\n",
            "Portuguese      228\n",
            "Italian         169\n",
            "Dutch           126\n",
            "Turkish         116\n",
            "Indonesian      115\n",
            "Name: bookId, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2) Ποιά είναι τα 10 βιβλία με τις περισσότερες σελίδες.\n",
        "sorted_by_pages = df_clean.sort_values('pages', key=lambda x: x)\n",
        "print(\"Books with most pages: \\n\", sorted_by_pages['bookId'][:10])\n",
        "\n",
        "# 3) Ποιά είναι τα 10 βιβλία με τις περισσότερες σελίδες.\n",
        "sorted_by_rating = df_clean.sort_values('ratingStar5', key=lambda x: x)\n",
        "print(\"Books with most 5 star ratings: \\n\", sorted_by_rating['bookId'][:10])\n",
        "\n",
        "# 5) Ποιοι είναι οι 10 συγγραφεις με τα περισσότερα βιβλία\n",
        "grouped_by_author = df_clean.groupby(['author']).count()\n",
        "sorted_by_author_books = grouped_by_author.sort_values(['bookId'], key=lambda x: x,ascending=False)\n",
        "print(\"Authors with most books: \\n\", sorted_by_author_books['bookId'][:10])\n",
        "\n",
        "# 6) Ποιοι είναι οι 10 συγγραφείς με τις περισσότερες κριτικές\n",
        "sorted_by_author_ratings = grouped_by_author.sort_values(['numRatings'], key=lambda x: x,ascending=False)\n",
        "print(\"Authors with most ratings: \\n\", sorted_by_author_ratings['numRatings'][:10])\n",
        "\n",
        "# 8) Ποιές είναι οι πιο συχνές γλώσσες που έχουν γραφτεί τα βιβλία\n",
        "grouped_by_language = df_clean.groupby('language')['bookId'].count()\n",
        "sorted_by_language = grouped_by_language.sort_values(ascending=False)\n",
        "print(\"Most popular languages: \\n\", sorted_by_language[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d429d8e",
      "metadata": {
        "id": "0d429d8e"
      },
      "source": [
        "## 3) Υλοποίηση Recommendation system (35%)\n",
        "Ο στόχος ενός τέτοιου συστήματος είναι (1) να προβλέψει τις αξιολογήσεις ενός χρήστη για τα\n",
        "βιβλία που δεν έχει διαβάσει ακόμα, και (2) να εμφανίσει ένα ταξινομημένη λίστα με τα κορυφαία\n",
        "Ν βιβλία για τα οποία πιστεύουμε ότι θα ήθελαν να μάθουν περισσότερα. ‘Ενας άλλος στόχος\n",
        "ενός Recommender είναι (3) να βοηθήσει τους χρήστες να ανακαλύψουν σχετικά βιβλία που δεν\n",
        "θα είχαν βρει διαφορετικά.\n",
        "Σε αυτό το ερώτημα θα χρειαστείτε τις στήλες\n",
        "* BookId\n",
        "* Description\n",
        "* Και μόνο όσες γραμμές έχουν γλώσσα “English”. \n",
        "\n",
        "Δημιουργήστε τον TF-IDF (Term Frequency - Inverse Document Frequency) πίνακα των\n",
        "unigrams και των bigrams για τη στήλη description (χρησιμοποιήστε την παράμετρο stop_word\n",
        "του TfidfVectorizer).\n",
        "\n",
        "**Cosine Similarity**: Η μετρική αυτή υπολογίζει την ομοιότητα μεταξύ δύο διανυσμάτων x,y,\n",
        "χρησιμοποιώντας τη γωνία μεταξύ τους (όταν η γωνία είναι 0 σημαίνει ότι τα x και y είναι ίσα , αν\n",
        "εξαιρέσουμε το μήκος τους). Διατρέξτε τον TF-IDF πίνακα και υπολογίστε το similarity καθενός\n",
        "βιβλίου με τα υπόλοιπα. Αποθηκεύστε σε ένα python dictionary τα 100 πιο όμοια βιβλία.\n",
        "\n",
        "Πρόβλεψη: Φτιάξτε μία συνάρτηση η οποία παίρνει σαν είσοδο ένα id και ένα ακέραιο αριθμό N,\n",
        "και επιστρέφει τα Ν πιο όμοια βιβλία.\n",
        "recommend(item_id = 4085439, num = 5)\n",
        "\n",
        "Η έξοδος της συνάρτησης να είναι της παρακάτω μορφής μορφής:\n",
        "```\n",
        "Recommending 5 books similar to: The Hunger Games\n",
        "-------------------------------------------------\n",
        "Recommended: NAME\n",
        "Description: DESCRIPTION\n",
        "(score:0.12235188993161432)\n",
        "Recommended: NAME\n",
        "Description: DESCRIPTION\n",
        "(score:0.12235188993161432 \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "123a21a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "123a21a3",
        "outputId": "07109e8d-b81a-439b-ddec-eab2eb2ed06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recommending 5 books similar to: The Madwoman of Chaillot \n",
            "-------------------------------------------------\n",
            "Recommended:  Oil!\n",
            "Description:  In Oil! Upton Sinclair fashioned a novel out of the oil scandals of the Harding administration, providing in the process a detailed picture of the development of the oil industry in Southern California. Bribery of public officials, class warfare, and international rivalry over oil production are the context for Sinclair's story of a genial independent oil developer and his son, whose sympathy with the oilfield workers and socialist organizers fuels a running debate with his father. Senators, small investors, oil magnates, a Hollywood film star, and a crusading evangelist people the pages of this lively novel.\n",
            "(score: 0.12945570857873878 )\n",
            "\n",
            "Recommended:  Land of Black Gold\n",
            "Description:  The classic graphic novel. Car engines have started spontaneously exploding all over the country . . . someone's been tampering with the oil! Tintin, with Thompson and Thompson at his side, sails on an oil tanker to the Middle East to track down the source of the faulty oil.\n",
            "(score: 0.10929036134517238 )\n",
            "\n",
            "Recommended:  The Prize: The Epic Quest for Oil, Money, and Power\n",
            "Description:  The Prize recounts the panoramic history of oil -- and the struggle for wealth and power that has always surrounded oil. This struggle has shaken the world economy, dictated the outcome of wars, and transformed the destiny of men and nations.The Prize is as much a history of the twentieth century as of the oil industry itself. The canvas of history is enormous -- from the drilling of the first well in Pennsylvania through two great world wars to the Iraqi invasion of Kuwait and Operation Desert Storm.\n",
            "(score: 0.10591966061778223 )\n",
            "\n",
            "Recommended:  Ill Wind\n",
            "Description:  It's the largest oil spill in history: a crashed supertanker in San Francisco Bay. Desperate to avert environmental damage—and a PR disaster—the multinational oil company releases an untested \"designer microbe\" to break up the spill.An \"oil-eating\" microbe, designed to consume anything made of petrocarbons: oil, gasoline, synthetic fabrics, and of course plastic.What the company doesn't realize is that their microbe propagates through the air. But when every car in the Bay Area turns up with an empty gas tank, they begin to suspect something is terribly wrong.And when, in just a few days, every piece of plastic in the world has dissolved, it's too late...\n",
            "(score: 0.0870708177870063 )\n",
            "\n",
            "Recommended:  The Call of the Wild\n",
            "Description:  First published in 1903, The Call of the Wild is regarded as Jack London's masterpiece. Based on London's experiences as a gold prospector in the Canadian wilderness and his ideas about nature and the struggle for existence, The Call of the Wild is a tale about unbreakable spirit and the fight for survival in the frozen Alaskan Klondike.\n",
            "(score: 0.07718245359707096 )\n",
            "\n",
            "Recommended:  The Book of Tea\n",
            "Description:  Now available in a gorgeous hardcover slipcase edition, this \"object d'art\" will be sure to add grace and elegance to tea shelves, coffee tables and bookshelves. A keepsake enjoyed by tea lovers for over a hundred years, The Book of Tea Classic Edition will enhance your enjoyment and understanding of the seemingly simple act of making and drinking tea.In 1906 in turn-of-the-century Boston, a small, esoteric book about tea was written with the intention of being read aloud in the famous salon of Isabella Gardner, Boston's most notorious socialite. It was authored by Okakura Kakuzo, a Japanese philosopher, art expert, and curator. Little known at the time, Kakuzo would emerge as one of the great thinkers of the early 20th century, a genius who was insightful, witty—and greatly responsible for bridging Western and Eastern cultures. Okakura had been taught at a young age to speak English and was more than capable of expressing to Westerners the nuances of tea and the Japanese Tea Ceremony.In The Book of Tea Classic Edition, he discusses such topics as Zen and Taoism, but also the secular aspects of tea and Japanese life. The book emphasizes how Teaism taught the Japanese many things; most importantly, simplicity. Kakuzo argues that tea-induced simplicity affected the culture, art and architecture of Japan.Nearly a century later, Kakuzo's The Book of Tea Classic Edition is still beloved the world over, making it an essential part of any tea enthusiast's collection. Interwoven with a rich history of Japanese tea and its place in Japanese society is a poignant commentary on Asian culture and our ongoing fascination with it, as well as illuminating essays on art, spirituality, poetry, and more. The Book of Tea Classic Edition is a delightful cup of enlightenment from a man far ahead of his time.\n",
            "(score: 0.07266048747654218 )\n",
            "\n",
            "Recommended:  Notes From The Tilt-A-Whirl: Wide-Eyed Wonder in God's Spoken World\n",
            "Description:  Product Description A visual, poetic exploration of the narrative nature of the world and the personality of the Poet behind it all.  When Nate Wilson looks at the world around him, he asks \"What is this place? Why is this place? Who approved it? Am I supposed to take it seriously?\" What could such an outlandish, fantastical world say about its Creator?  In these sparkling chapters, Wilson gives an aesthetic examination of the ways in which humanity has tried to make sense of this overwhelming carnival ride of a world. He takes a whimsical, thought-provoking look at everything from the \"magic\" of quantum physics, to nature's absurdities, to the problem of evil, evolution and hell. These frequently humorous, and uniquely beautiful portraits express reality unknown to many Christians-the reality of God's story unfolding around and among us. As the author says, \"Welcome to His poem. His play. His novel. His comedy. Let the pages flick your thumbs.\"\n",
            "(score: 0.07259233668309419 )\n",
            "\n",
            "Recommended:  Sunlight on Cold Water\n",
            "Description:  Gilles Latiner is thirty-five, attractive, with a beautiful mistress, and a job in Paris as a journalist. He seems to have all that life can offer. But suddenly he is overwhelmed by despair. Nothing seems worth while. In panic at his boredom, and hating Eloise, his model girlfriend, he flees for some peace to his sister and her husband in the provinces.Here he meets Nathalie, the wife of a country lawyer. She falls deeply in love with him, a passion to which he soon responds. But back in Paris her innate goodness contrasts oddly with the frivolity of Gilles’s life. Soon it seems as if their relationship is doomed, as if their happiness is a mere gleam of sunlight on cold water…”\n",
            "(score: 0.06999357593544582 )\n",
            "\n",
            "Recommended:  The Legend of Zelda: Twilight Princess, Vol. 1\n",
            "Description:  Once upon a time, wizards tried to conquer the Sacred Realm of Hyrule. The Spirits of Light sealed the wizards' power within the Shadow Crystal and banished them to the Twilight Realm beyond the Mirror of Twilight. Now, an evil menace is trying to find Midna, princess of the Twilight Realm, and the fragments of the Shadow Crystal to gain the power to rule over both the Twilight Realm and the World of Light.\n",
            "(score: 0.06975356944990799 )\n",
            "\n",
            "Recommended:  The Tea Dragon Society\n",
            "Description:  From the award-winning author of Princess Princess Ever After comes The Tea Dragon Society, a charming all-ages book that follows the story of Greta, a blacksmith apprentice, and the people she meets as she becomes entwined in the enchanting world of tea dragons. After discovering a lost tea dragon in the marketplace, Greta learns about the dying art form of tea dragon care-taking from the kind tea shop owners, Hesekiel and Erik. As she befriends them and their shy ward, Minette, Greta sees how the craft enriches their lives—and eventually her own.\n",
            "(score: 0.06918346056150523 )\n",
            "\n",
            "\n",
            "Recommending 5 books similar to: The Madwoman of Chaillot \n",
            "-------------------------------------------------\n",
            "Recommended:  The Tempering Blaze\n",
            "Description:  With the Boston Tea Party in December of 1773, the match was struck, kindling the flames of the American Revolution. This is the stirring conclusion to a gripping saga of love and war, betrayal and forgiveness.And here are the people who struggle with unexpected love and God's call in their lives.SUSANNAH HAYNES is held captive in Boston by the arrogant British Lieutenant Alex Fontaine, while her husband Dan runs for his life.TED HARRINGTON, Susannah's brother, who risked all to throw in his lost with the Colonies, tries (with his wife Jane) to escape the sentence of traitor to the Crown.YANCY CURTIS, who barely escaped with his life on the night of the Boston Tea Party, finds himself exiled in the mountains of Virginia.Young postrider BEN HAYNES, caught up in the fervor of patriotism, nearly misses God's will for his life, because of his obsession with the righteous cause of freedom.\n",
            "(score: 0.017718167360302284 )\n",
            "\n",
            "Recommended:  Pegasus and the Fight for Olympus\n",
            "Description:  Reborn as the Flame of Olympus, Emily must embrace her new powers and responsibilities. But all she wants is to return to her world, where her father remains a prisoner of the sinister government agency, the CRU.\n",
            "(score: 0.017040841891401256 )\n",
            "\n",
            "Recommended:  The Man Without Qualities\n",
            "Description:  Set in Vienna on the eve of World War I, this great novel of ideas tells the story of Ulrich, ex-soldier and scientist, seducer and skeptic, who finds himself drafted into the grandiose plans for the 70th jubilee of the Emperor Franz Josef. This new translation - published in two elegant volumes - is the first to present Musil's complete text, including material that remained unpublished during his lifetime.\n",
            "(score: 0.016997213580785705 )\n",
            "\n",
            "Recommended:  Topdog/Underdog\n",
            "Description:  A darkly comic fable of brotherly love and family identity is Suzan-Lori Parks latest riff on the way we are defined by history. The play tells the story of Lincoln and Booth, two brothers whose names were given to them as a joke, foretelling a lifetime of sibling rivalry and resentment. Haunted by the past, the brothers are forced to confront the shattering reality of their future.\n",
            "(score: 0.016949372256065735 )\n",
            "\n",
            "Recommended:  A New Christianity for a New World: Why Traditional Faith is Dying How a New Faith is Being Born\n",
            "Description:  In his bestselling book Why Christianity Must Change or Die, Bishop John Shelby Spong described the toxins that are poisoning the Church. Now he offers the antidote, calling Christians everywhere into a new and radical reformation for a new age. Spong looks beyond traditional boundaries to open new avenues and a new vocabulary into the Holy, proposing a Christianity premised upon justice, love, and the rise of a new humanity -- a vision of the power that might be.\n",
            "(score: 0.013905668226768446 )\n",
            "\n",
            "Recommended:  Berlin, Vol. 1: City of Stones\n",
            "Description:  Berlin: City of Stones presents the first part of Jason Lutes' captivating trilogy, set in the twilight years of Germany's Weimar Republic. Kurt Severing, a journalist, and Marthe Muller, an art student, are the central figures in a broad cast of characters intertwined with the historical events unfolding around them. City of Stones covers eight months in Berlin, from September 1928 to May Day, 1929, meticulously documenting the hopes and struggles of its inhabitants as their future is darkened by a glowing shadow.\n",
            "(score: 0.013662172110703055 )\n",
            "\n",
            "Recommended:  Batman: Tales of the Demon\n",
            "Description:  Ra's al Ghul, longlived ecoterrorist and founder of the League of Assassins, has proven through the years to be one of Batman's most formidable adversaries. Accompanied by his beautiful daughter Talia, Ra's has proven elusive, tempting and very, very dangerous.This volume collects the earliest Ra's a Ghul stories, all written by long time BATMAN author and editor Denny O'Neil, aided by some of the Batman's bestknown artists Neal Adams, Michael Golden, Irv Novick, Bob Brown and Dick Giordano. There's also an introduction by Sam Hamm, screenwriter of 1989's Batman.\n",
            "(score: 0.013451608170226293 )\n",
            "\n",
            "Recommended:  Family Reunion\n",
            "Description:  Family Reunion is the third and final entry to the Webster Fields saga, that began with Princess Ces'alena, continued on with Gold Raven, and now - the reunion. The family comes full circle together, succeeding in the greatest find of all, lover, children, friends and daughter! Now finally, their dream is realized as Manny fulfills his promise to Lena, taking her back to Makia Island; back to her home, back to her father. But, only for a visit! Problem, her father has other plans. Come join us in the concluding leg of their journey, into a story that many have said, they cannot, will not, forget!\n",
            "(score: 0.013118311979598788 )\n",
            "\n",
            "Recommended:  Swordsman's Legacy\n",
            "Description:  In need of a break from work and some recent near-death adventures, archaeologist Annja Creed visits France to indulge one of her greatest fantasies: finding D'Artagnan's lost sword. The rapier was a gift from the reigning monarch and has been missing since the seventeenth century.And Ascher Vallois, one of Annja's treasure-hunting friends, believes he has located the site of the relic.But when Annja meets with Vallois, she learns that he's made a huge sacrifice to protect the sword and its secret from a relic hunter. Annja discovers that the artifact holds the key to a fortune. And the man won't stop until he gets everything he wants—including Annja.\n",
            "(score: 0.013082962214456071 )\n",
            "\n",
            "Recommended:  Husband\n",
            "Description:  Two years of bitter loneliness have passed. The woman I love is now the woman I hate. I lose myself in beautiful women every night and tell myself that fortune reading was nothing but a scam.But then her mother asks me to marry her.I say yes.Now this woman will be mine forever.Maybe the prophecy really is true. I’m committing to a woman that will never love me in return.But it was better to be her husband than allow someone else to take my place. It was better to conquer her body every night than be lonely with someone else.Much better.\n",
            "(score: 0.01305082831326205 )\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Keep only the book Ids and their respective descriptions \n",
        "# ΤHN book_descriptions ΧΡΕΙΑΖΕΤΑΙ H recommend() function \n",
        "book_descriptions = df_clean.loc[ \n",
        "    (df_clean['language'] == 'English'),['bookId','description']].reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "unigram_vectorizer = TfidfVectorizer(\n",
        "    token_pattern = r'\\b(?![A-Za-z]\\b)[A-Za-z]+\\b', #ignore numbers and such\n",
        "    #min_df = 0.0014, # I exclude rare words, as they have little contribution to overarching theme of a book,might change it later\n",
        "    ngram_range=(1, 1), # unigrams and bigrams\n",
        "    strip_accents='unicode', # probably won't need that since we are only in English but you never know\n",
        "    stop_words='english', # ignore stop words \n",
        ")\n",
        "\n",
        "bigram_vectorizer = TfidfVectorizer(\n",
        "    token_pattern = r'\\b(?![A-Za-z]\\b)[A-Za-z]+\\b', #ignore numbers and such\n",
        "    #min_df = 0.0014, # I exclude rare words, as they have little contribution to overarching theme of a book,might change it later\n",
        "    ngram_range=(2, 2), # unigrams and bigrams\n",
        "    strip_accents='unicode', # probably won't need that since we are only in English but you never know\n",
        "    stop_words='english', # ignore stop words \n",
        ")\n",
        "\n",
        "#rows = len(book_descriptions.index)\n",
        "\n",
        "#matrix = vectorizer.fit_transform(book_descriptions['description'])\n",
        "\n",
        "unigram_matrix = unigram_vectorizer.fit_transform(book_descriptions['description'])\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(book_descriptions['description'])\n",
        "\n",
        "unigram_similarities = cosine_similarity(unigram_matrix) # calculate the similarity matrix\n",
        "np.fill_diagonal(unigram_similarities,-1) # remove the self similarities\n",
        "#unigram_flat = unigram_similarities.flatten()\n",
        " \n",
        "bigram_similarities = cosine_similarity(bigram_matrix) # calculate the similarity matrix\n",
        "np.fill_diagonal(bigram_similarities,-1) # remove the self similarities\n",
        "#bigram_flat = bigram_similarities.flatten()\n",
        "\n",
        "\n",
        "# creating pairs with the 100 most similar books according to the unigrams\n",
        "unigrams_pairs = []\n",
        "for i in range(len(unigram_similarities)):\n",
        "    array = unigram_similarities[i].copy()\n",
        "    for j in range(100): # for 100 times\n",
        "        index = np.argmax(array) # get the largest remaining similarity in the flat matrix\n",
        "        unigrams_pairs.append((i,index,array[index])) # save the similarity pair\n",
        "        array[index] = -1 # get remove it from the matrix so as to not find it again\n",
        "        \n",
        "unigram_dictionary = {}\n",
        "i = 0\n",
        "for (index1,index2,sim) in unigrams_pairs:\n",
        "    unigram_dictionary[i] = (book_descriptions['bookId'][index1],book_descriptions['bookId'][index2],sim)\n",
        "    i = i+1\n",
        "  \n",
        "# creating pairs with the 100 most similar books according to the bigrams\n",
        "bigrams_pairs = []\n",
        "for i in range(len(bigram_similarities)):\n",
        "    array = bigram_similarities[i].copy()\n",
        "    for j in range(100): # for 100 times\n",
        "        index = np.argmax(array) # get the largest remaining similarity in the flat matrix\n",
        "        bigrams_pairs.append((i,index,array[index])) # save the similarity pair\n",
        "        array[index] = -1 # get remove it from the matrix so as to not find it again\n",
        "        \n",
        "bigram_dictionary = {}\n",
        "i = 0\n",
        "for (index1,index2,sim) in bigrams_pairs:\n",
        "    bigram_dictionary[i] = (book_descriptions['bookId'][index1],book_descriptions['bookId'][index2],sim)\n",
        "    i = i+1\n",
        "\n",
        "def recommend(bookId, findNsimilar,dictionary):\n",
        "    title = df_clean.loc[df_clean['bookId'] == bookId, 'title'].values[0]\n",
        "    print(\"\\nRecommending 5 books similar to:\", title, \"\\n-------------------------------------------------\")\n",
        "    row = (book_descriptions[book_descriptions['bookId'] == bookId].index.tolist())[0]\n",
        "    for i in range(findNsimilar):\n",
        "        bookId = dictionary[row*100+i][1]\n",
        "        recommended_title = df_clean.loc[df_clean['bookId'] == bookId, 'title'].values[0]\n",
        "        recommended_description = df_clean.loc[df_clean['bookId'] == bookId, 'description'].values[0]\n",
        "        print(\"Recommended: \", recommended_title)\n",
        "        print(\"Description: \", recommended_description,)\n",
        "        print(\"(score:\", dictionary[row*100+i][2], \")\\n\")\n",
        "        \n",
        "recommend('591965.The_Madwoman_of_Chaillot', 10, unigram_dictionary)\n",
        "recommend('591965.The_Madwoman_of_Chaillot', 10, bigram_dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f2edc0",
      "metadata": {
        "id": "64f2edc0"
      },
      "source": [
        "## 4) Υλοποίηση Κατηγοριοποίησης (Classification) (35%)\n",
        "Χρησιμοποιήστε την στήλη genreSingle, βρείτε τα 10 πιο συχνά genres και κρατήστε σε ένα νέο\n",
        "dataframe τα βιβλία εκείνα που ανήκουν σε αυτες τις 10 πιο συχνές κατηγορίες. Θα χρειαστείτε\n",
        "το bookId το description και το genreSingle. Στην συνέχεια καθαρίστε την στήλη description\n",
        "χρησιμοποιώντας τις μεθόδους που είδαμε στα φροντιστήρια (πχ αφαίρεση σημείων στίξης,\n",
        "μετατροπή όλων των χαρακτήρων σε πεζά, κ.α.). Εφαρμόστε την μέθοδο word2vec για τα\n",
        "descriptions και στην συνέχεια με την χρήση των embeddings να υπολογίσετε για κάθε\n",
        "description ένα διάνυσμα με 200-300 τιμές (features) - αυτό θα είναι ο μέσος όρος των\n",
        "embeddings των λέξεων από τις οποίες αποτελείται το description.\n",
        "\n",
        "Χρησιμοποιήστε τη βιβλιοθήκη pickle της Python για να αποθηκεύσετε τα χαρακτηριστικά σε\n",
        "αρχεία *.pkl . Με αυτό τον τρόπο δεν χρειάζεται να υπολογίζονται από την αρχή τα\n",
        "χαρακτηριστικά κάθε φορά που τρέχετε το πρόγραμμά σας, αλλά μπορείτε μόνο να τα φορτωνεται\n",
        "στην μνήμη χρησιμοποιώντας την αντίστοιχη μέθοδο load.\n",
        "Χωρίστε το σύνολο των δεδομένων σε train (80%) και test (20%) χρησιμοποιώντας την μέθοδο\n",
        "train_test_split() της βιβλιοθήκης sklearn.\n",
        "\n",
        "Σε αυτό το ερώτημα θα πρέπει το πρόγραμμα σας να μπορεί να βρει τις κατηγορίες (genre) του\n",
        "συνόλου δοκιμής (test) χρησιμοποιώντας τις παρακάτω μεθόδους Classification:\n",
        "* Naive Bayes\n",
        "* Support Vector Machines (SVM, να πειραματιστείτε με τις παραμέτρους kernel (rbf, linear), c\n",
        "και gamma. H επιλογή των παραμέτρων μπορεί να γίνει και με GridSearchCV)\n",
        "* Random Forests\n",
        "\n",
        "Ολα τα παραπάνω μοντέλα θα εκπαιδευτούν MONO στο σύνολο train και θα αξιολογηθούν στο\n",
        "σύνολο test. Επίσης θα πρέπει να αξιολογήσετε και να καταγράψετε την απόδοση κάθε μεθόδου\n",
        "χρησιμοποιώντας 10-fold Cross Validation χρησιμοποιώντας τις παρακάτω μετρικές:\n",
        "* Precision / Recall / F-Measure\n",
        "* Accuracy\n",
        "\n",
        "Στο τέλος να ετοιμάσετε ένα πίνακα με τα αποτελέσματα των πειραμάτων σας για κάθε\n",
        "μετρική/παράμετρο που χρησιμοποιήσατε."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2431a94e",
      "metadata": {
        "id": "2431a94e"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "zaSO3IvxZKBe",
      "metadata": {
        "id": "zaSO3IvxZKBe"
      },
      "outputs": [],
      "source": [
        "ENGLISH_STOP_WORDS = set([\n",
        "    'a',\n",
        "    'about',\n",
        "    'above',\n",
        "    'across',\n",
        "    'after',\n",
        "    'afterwards',\n",
        "    'again',\n",
        "    'against',\n",
        "    'ain',\n",
        "    'all',\n",
        "    'almost',\n",
        "    'alone',\n",
        "    'along',\n",
        "    'already',\n",
        "    'also',\n",
        "    'although',\n",
        "    'always',\n",
        "    'am',\n",
        "    'among',\n",
        "    'amongst',\n",
        "    'amoungst',\n",
        "    'amount',\n",
        "    'an',\n",
        "    'and',\n",
        "    'another',\n",
        "    'any',\n",
        "    'anyhow',\n",
        "    'anyone',\n",
        "    'anything',\n",
        "    'anyway',\n",
        "    'anywhere',\n",
        "    'are',\n",
        "    'aren',\n",
        "    'around',\n",
        "    'as',\n",
        "    'at',\n",
        "    'back',\n",
        "    'be',\n",
        "    'became',\n",
        "    'because',\n",
        "    'become',\n",
        "    'becomes',\n",
        "    'becoming',\n",
        "    'been',\n",
        "    'before',\n",
        "    'beforehand',\n",
        "    'behind',\n",
        "    'being',\n",
        "    'below',\n",
        "    'beside',\n",
        "    'besides',\n",
        "    'between',\n",
        "    'beyond',\n",
        "    'bill',\n",
        "    'both',\n",
        "    'bottom',\n",
        "    'but',\n",
        "    'by',\n",
        "    'call',\n",
        "    'can',\n",
        "    'cannot',\n",
        "    'cant',\n",
        "    'co',\n",
        "    'con',\n",
        "    'could',\n",
        "    'couldn',\n",
        "    'couldnt',\n",
        "    'cry',\n",
        "    'd',\n",
        "    'de',\n",
        "    'describe',\n",
        "    'detail',\n",
        "    'did',\n",
        "    'didn',\n",
        "    'do',\n",
        "    'does',\n",
        "    'doesn',\n",
        "    'doing',\n",
        "    'don',\n",
        "    'done',\n",
        "    'down',\n",
        "    'due',\n",
        "    'during',\n",
        "    'each',\n",
        "    'eg',\n",
        "    'eight',\n",
        "    'either',\n",
        "    'eleven',\n",
        "    'else',\n",
        "    'elsewhere',\n",
        "    'empty',\n",
        "    'enough',\n",
        "    'etc',\n",
        "    'even',\n",
        "    'ever',\n",
        "    'every',\n",
        "    'everyone',\n",
        "    'everything',\n",
        "    'everywhere',\n",
        "    'except',\n",
        "    'few',\n",
        "    'fifteen',\n",
        "    'fify',\n",
        "    'fill',\n",
        "    'find',\n",
        "    'fire',\n",
        "    'first',\n",
        "    'five',\n",
        "    'for',\n",
        "    'former',\n",
        "    'formerly',\n",
        "    'forty',\n",
        "    'found',\n",
        "    'four',\n",
        "    'from',\n",
        "    'front',\n",
        "    'full',\n",
        "    'further',\n",
        "    'get',\n",
        "    'give',\n",
        "    'go',\n",
        "    'had',\n",
        "    'hadn',\n",
        "    'has',\n",
        "    'hasn',\n",
        "    'hasnt',\n",
        "    'have',\n",
        "    'haven',\n",
        "    'having',\n",
        "    'he',\n",
        "    'hence',\n",
        "    'her',\n",
        "    'here',\n",
        "    'hereafter',\n",
        "    'hereby',\n",
        "    'herein',\n",
        "    'hereupon',\n",
        "    'hers',\n",
        "    'herself',\n",
        "    'him',\n",
        "    'himself',\n",
        "    'his',\n",
        "    'how',\n",
        "    'however',\n",
        "    'hundred',\n",
        "    'i',\n",
        "    'ie',\n",
        "    'if',\n",
        "    'in',\n",
        "    'inc',\n",
        "    'indeed',\n",
        "    'interest',\n",
        "    'into',\n",
        "    'is',\n",
        "    'isn',\n",
        "    'it',\n",
        "    'its',\n",
        "    'itself',\n",
        "    'just',\n",
        "    'keep',\n",
        "    'last',\n",
        "    'latter',\n",
        "    'latterly',\n",
        "    'least',\n",
        "    'less',\n",
        "    'll',\n",
        "    'ltd',\n",
        "    'm',\n",
        "    'ma',\n",
        "    'made',\n",
        "    'many',\n",
        "    'may',\n",
        "    'me',\n",
        "    'meanwhile',\n",
        "    'might',\n",
        "    'mightn',\n",
        "    'mill',\n",
        "    'mine',\n",
        "    'more',\n",
        "    'moreover',\n",
        "    'most',\n",
        "    'mostly',\n",
        "    'move',\n",
        "    'much',\n",
        "    'must',\n",
        "    'mustn',\n",
        "    'my',\n",
        "    'myself',\n",
        "    'name',\n",
        "    'namely',\n",
        "    'needn',\n",
        "    'neither',\n",
        "    'never',\n",
        "    'nevertheless',\n",
        "    'next',\n",
        "    'nine',\n",
        "    'no',\n",
        "    'nobody',\n",
        "    'none',\n",
        "    'noone',\n",
        "    'nor',\n",
        "    'not',\n",
        "    'nothing',\n",
        "    'now',\n",
        "    'nowhere',\n",
        "    'o',\n",
        "    'of',\n",
        "    'off',\n",
        "    'often',\n",
        "    'on',\n",
        "    'once',\n",
        "    'one',\n",
        "    'only',\n",
        "    'onto',\n",
        "    'or',\n",
        "    'other',\n",
        "    'others',\n",
        "    'otherwise',\n",
        "    'our',\n",
        "    'ours',\n",
        "    'ourselves',\n",
        "    'out',\n",
        "    'over',\n",
        "    'own',\n",
        "    'part',\n",
        "    'per',\n",
        "    'perhaps',\n",
        "    'please',\n",
        "    'put',\n",
        "    'rather',\n",
        "    're',\n",
        "    's',\n",
        "    'same',\n",
        "    'see',\n",
        "    'seem',\n",
        "    'seemed',\n",
        "    'seeming',\n",
        "    'seems',\n",
        "    'serious',\n",
        "    'several',\n",
        "    'shan',\n",
        "    'she',\n",
        "    'should',\n",
        "    'shouldn',\n",
        "    'show',\n",
        "    'side',\n",
        "    'since',\n",
        "    'sincere',\n",
        "    'six',\n",
        "    'sixty',\n",
        "    'so',\n",
        "    'some',\n",
        "    'somehow',\n",
        "    'someone',\n",
        "    'something',\n",
        "    'sometime',\n",
        "    'sometimes',\n",
        "    'somewhere',\n",
        "    'still',\n",
        "    'such',\n",
        "    'system',\n",
        "    't',\n",
        "    'take',\n",
        "    'ten',\n",
        "    'than',\n",
        "    'that',\n",
        "    'the',\n",
        "    'their',\n",
        "    'theirs',\n",
        "    'them',\n",
        "    'themselves',\n",
        "    'then',\n",
        "    'thence',\n",
        "    'there',\n",
        "    'thereafter',\n",
        "    'thereby',\n",
        "    'therefore',\n",
        "    'therein',\n",
        "    'thereupon',\n",
        "    'these',\n",
        "    'they',\n",
        "    'thick',\n",
        "    'thin',\n",
        "    'third',\n",
        "    'this',\n",
        "    'those',\n",
        "    'though',\n",
        "    'three',\n",
        "    'through',\n",
        "    'throughout',\n",
        "    'thru',\n",
        "    'thus',\n",
        "    'to',\n",
        "    'together',\n",
        "    'too',\n",
        "    'top',\n",
        "    'toward',\n",
        "    'towards',\n",
        "    'twelve',\n",
        "    'twenty',\n",
        "    'two',\n",
        "    'un',\n",
        "    'under',\n",
        "    'until',\n",
        "    'up',\n",
        "    'upon',\n",
        "    'us',\n",
        "    've',\n",
        "    'very',\n",
        "    'via',\n",
        "    'was',\n",
        "    'wasn',\n",
        "    'we',\n",
        "    'well',\n",
        "    'were',\n",
        "    'weren',\n",
        "    'what',\n",
        "    'whatever',\n",
        "    'when',\n",
        "    'whence',\n",
        "    'whenever',\n",
        "    'where',\n",
        "    'whereafter',\n",
        "    'whereas',\n",
        "    'whereby',\n",
        "    'wherein',\n",
        "    'whereupon',\n",
        "    'wherever',\n",
        "    'whether',\n",
        "    'which',\n",
        "    'while',\n",
        "    'whither',\n",
        "    'who',\n",
        "    'whoever',\n",
        "    'whole',\n",
        "    'whom',\n",
        "    'whose',\n",
        "    'why',\n",
        "    'will',\n",
        "    'with',\n",
        "    'within',\n",
        "    'without',\n",
        "    'won',\n",
        "    'would',\n",
        "    'wouldn',\n",
        "    'y',\n",
        "    'yet',\n",
        "    'you',\n",
        "    'your',\n",
        "    'yours',\n",
        "    'yourself',\n",
        "    'yourselves'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "PTVsGS2KXCpi",
      "metadata": {
        "id": "PTVsGS2KXCpi"
      },
      "outputs": [],
      "source": [
        "# cleaning up the dataframe\n",
        "df_genre_counts = df_clean.groupby('genreSingle')[['bookId']].count()\n",
        "top_10 = df_genre_counts.sort_values(by='bookId', ascending=False)[:10]\n",
        "top_10_with_genre = top_10.reset_index()\n",
        "\n",
        "new_df = df_clean.drop(df_clean[~df_clean['genreSingle'].isin(top_10_with_genre['genreSingle'])].index)\n",
        "new_df = new_df.loc[ \n",
        "    (new_df['language'] == 'English')].reset_index(drop=True)\n",
        "\n",
        "keep = ['bookId', 'genreSingle','description']\n",
        "new_df = new_df.drop(columns=[col for col in new_df.columns if col not in keep])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3biEBcOfXVVi",
      "metadata": {
        "id": "3biEBcOfXVVi"
      },
      "outputs": [],
      "source": [
        "# cleaning up the descriptions\n",
        "clean_descriptions = []\n",
        "for description in new_df['description']:\n",
        "    # split the description in anything that is not an character and return every word that is not a stop word and not empty\n",
        "    tokens = [word for word in re.split(r'\\W+|\\d+', description.lower()) if word and word not in ENGLISH_STOP_WORDS] \n",
        "    clean_descriptions.append(tokens)\n",
        "\n",
        "# creating the vocabulary as a set as to not allow duplicates\n",
        "vocabulary = list(set(word for description in clean_descriptions for word in description))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "xUprB0XtD8s4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUprB0XtD8s4",
        "outputId": "29a45dfc-43a0-4360-99f8-b1864183da95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(clean_descriptions,min_count=1)\n",
        "word_embeddings = model.wv\n",
        "\n",
        "file_path = 'word_embeddings.pickle'\n",
        "with open(file_path, 'wb') as f:\n",
        "    pickle.dump(word_embeddings, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "kNDDFt7zaXna",
      "metadata": {
        "id": "kNDDFt7zaXna"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the embedding \n",
        "with open('word_embeddings.pickle', 'rb') as f:\n",
        "    loaded_embeddings = pickle.load(f)\n",
        "\n",
        "# Calculate the description matrices\n",
        "description_vectors = []\n",
        "for description in clean_descriptions:\n",
        "    description_vector = 0\n",
        "    for word in description:\n",
        "        vector = word_embeddings[word]\n",
        "        description_vector = description_vector + vector\n",
        "    description_vectors.append(description_vector/len(description))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "uFUCBZxognYn",
      "metadata": {
        "id": "uFUCBZxognYn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(description_vectors,new_df['genreSingle'],test_size=0.2)\n",
        "\n",
        "#print(len(description_vectors),len(clean_descriptions))\n",
        "#new_df.info(verbose=True)\n",
        "#print(len(x_train),len(x_test),len(y_train),len(y_test))\n",
        "\n",
        "# Ready for testing!\n",
        "# Σε αυτό το ερώτημα θα πρέπει το πρόγραμμα σας να μπορεί να βρει τις κατηγορίες (genre) του συνόλου δοκιμής (test) χρησιμοποιώντας τις παρακάτω μεθόδους Classification:\n",
        "\n",
        "#     Naive Bayes\n",
        "#     Support Vector Machines (SVM, να πειραματιστείτε με τις παραμέτρους kernel (rbf, linear), c και gamma. H επιλογή των παραμέτρων μπορεί να γίνει και με GridSearchCV)\n",
        "#     Random Forests\n",
        "\n",
        "# Ολα τα παραπάνω μοντέλα θα εκπαιδευτούν MONO στο σύνολο train και θα αξιολογηθούν στο σύνολο test. Επίσης θα πρέπει να αξιολογήσετε και να καταγράψετε την απόδοση κάθε μεθόδου χρησιμοποιώντας 10-fold Cross Validation χρησιμοποιώντας τις παρακάτω μετρικές:\n",
        "\n",
        "#     Precision / Recall / F-Measure\n",
        "#     Accuracy\n",
        "\n",
        "# Στο τέλος να ετοιμάσετε ένα πίνακα με τα αποτελέσματα των πειραμάτων σας για κάθε μετρική/παράμετρο που χρησιμοποιήσατε.\n",
        "\n",
        "# svc = SVC(C=347, kernel='linear', gamma=0.04, tol=1.7)\n",
        "# svc.fit(x_sample,y_sample)\n",
        "# prediction = svc.predict(x_t_sample)\n",
        "# accuracy = accuracy_score(y_t_sample,prediction)\n",
        "# print(\"Linear: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "uPPkzdAekpOu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPPkzdAekpOu",
        "outputId": "ead5184c-fdcf-4b0f-bbbc-73ed150d0c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf', 'tol': 0.1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#SVM\n",
        "#HyperParameterSearch for Linear\n",
        "model = SVC()\n",
        "space = dict()\n",
        "space['C'] = [1e-5, 1e-3, 1e-1, 1, 10, 100, 1000]\n",
        "space['kernel'] = ['linear','rbf']\n",
        "space['gamma'] = [1e-5, 1e-3, 1e-1, 1, 10, 100, 1000]\n",
        "space['tol'] = [1e-5, 1e-3, 1e-1, 1, 10, 100, 1000]\n",
        "\n",
        "#Search\n",
        "search = GridSearchCV(model,space,scoring='accuracy',n_jobs=-1,cv=5)\n",
        "result = search.fit(x_train[:1000],y_train[:1000])\n",
        "print(search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "X5VZ7TNg6HuQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5VZ7TNg6HuQ",
        "outputId": "0613d19c-b9f9-4015-a0a7-4c30ec0cf9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Naive Bayes : \n",
            "\n",
            "Accuracy:  0.3432874392526024\n",
            "\n",
            "Precision:  0.3432874392526024\n",
            "\n",
            "Recall:  0.3432874392526024\n",
            "\n",
            "F-Measure:  0.3432874392526024\n",
            "\n",
            " Support Vector Machines : \n",
            "\n",
            "Accuracy:  0.528340911193175\n",
            "\n",
            "Precision:  0.528340911193175\n",
            "\n",
            "Recall:  0.528340911193175\n",
            "\n",
            "F-Measure:  0.528340911193175\n",
            "\n",
            " Random Forests : \n",
            "\n",
            "Accuracy:  0.48600017076867247\n",
            "\n",
            "Precision:  0.48600017076867247\n",
            "\n",
            "Recall:  0.48600017076867247\n",
            "\n",
            "F-Measure:  0.4860001707686726\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# ‘accuracy’\n",
        "# ‘f1_micro’\n",
        "# ‘precision’ etc. metrics.precision_score suffixes apply as with ‘f1’\n",
        "# ‘recall’ etc. metrics.recall_score suffixes apply as with ‘f1’\n",
        "\n",
        "estimators = {\n",
        "    0:('Naive Bayes',GaussianNB()),\n",
        "    1:('Support Vector Machines',SVC(C=10, kernel='rbf', gamma=0.1, tol=0.1)),\n",
        "    2:('Random Forests',RandomForestClassifier())\n",
        "}\n",
        "scoring = ['accuracy', 'f1_micro','precision_micro', 'recall_micro']\n",
        "matrix = []\n",
        "for i in range(len(estimators)):\n",
        "    cv_results = cross_validate(estimators[i][1], x_train, y_train, cv=10, scoring=scoring)\n",
        "    # Print the cross-validated scores\n",
        "    # scoresdict of float arrays of shape (n_splits,) Array of scores of the estimator for each run of the cross validation.\n",
        "    # Suffix _score in test_score changes to a specific metric \n",
        "    accuracy = cv_results['test_accuracy'].mean()\n",
        "    precision = cv_results['test_precision_micro'].mean()\n",
        "    recall = cv_results['test_recall_micro'].mean()\n",
        "    f_measure = cv_results['test_f1_micro'].mean()\n",
        "    matrix.append([accuracy,precision,recall,f_measure])\n",
        "    print(\"\\n\",estimators[i][0],\": \")\n",
        "    print(\"\\nAccuracy: \", accuracy)\n",
        "    print(\"\\nPrecision: \", precision)\n",
        "    print(\"\\nRecall: \", recall)\n",
        "    print(\"\\nF-Measure: \", f_measure)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FnsbV1KIKlT",
        "outputId": "06077e58-391f-4888-def6-7f29d4446639"
      },
      "id": "5FnsbV1KIKlT",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.3432874392526024, 0.3432874392526024, 0.3432874392526024, 0.3432874392526024], [0.528340911193175, 0.528340911193175, 0.528340911193175, 0.528340911193175], [0.48600017076867247, 0.48600017076867247, 0.48600017076867247, 0.4860001707686726]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}